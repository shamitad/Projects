{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1247638</td>\n",
       "      <td>Triple W to Feature DFree at CES 2019 - First ...</td>\n",
       "      <td>jan. according to the u. s. national institute...</td>\n",
       "      <td>2019-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1247639</td>\n",
       "      <td>Pundi X Coin Review: Should You Invest in the ...</td>\n",
       "      <td>the npxs token developer aim to have ethereum ...</td>\n",
       "      <td>2019-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1247641</td>\n",
       "      <td>Pundi X 1-Day Trading Volume Reaches $1.04 Mil...</td>\n",
       "      <td>pundi x (currency:npxs) traded 2. u dollar dur...</td>\n",
       "      <td>2019-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1247642</td>\n",
       "      <td>Pundi X Hits Market Capitalization of $74.35 M...</td>\n",
       "      <td>pundi x (currency:npxs) traded 0. pm eastern o...</td>\n",
       "      <td>2019-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1247643</td>\n",
       "      <td>Pundi X (NPXS) Market Cap Reaches $73.89 Million</td>\n",
       "      <td>pundi x (currency:npxs) traded down 0. u dolla...</td>\n",
       "      <td>2019-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11182</th>\n",
       "      <td>1388717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>microsoft for the mixed reality partnership pr...</td>\n",
       "      <td>2019-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11183</th>\n",
       "      <td>1388718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ar (ar) app that convert any room to a virtual...</td>\n",
       "      <td>2019-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11184</th>\n",
       "      <td>1388738</td>\n",
       "      <td>Intercellular is an Educational Experience Tak...</td>\n",
       "      <td>there nothing like a good educational vr (vr) ...</td>\n",
       "      <td>2019-02-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11185</th>\n",
       "      <td>1388746</td>\n",
       "      <td>Wonderfall Mixes VR and Actual Reality For An ...</td>\n",
       "      <td>it got a lot of vr and ar exhibit right now, n...</td>\n",
       "      <td>2019-01-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11186</th>\n",
       "      <td>1388760</td>\n",
       "      <td>VR and AR Market Forecast and Future Projections</td>\n",
       "      <td>immersive technology adoption rate have steadi...</td>\n",
       "      <td>2019-01-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11187 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "0      1247638  Triple W to Feature DFree at CES 2019 - First ...   \n",
       "1      1247639  Pundi X Coin Review: Should You Invest in the ...   \n",
       "2      1247641  Pundi X 1-Day Trading Volume Reaches $1.04 Mil...   \n",
       "3      1247642  Pundi X Hits Market Capitalization of $74.35 M...   \n",
       "4      1247643   Pundi X (NPXS) Market Cap Reaches $73.89 Million   \n",
       "...        ...                                                ...   \n",
       "11182  1388717                                                NaN   \n",
       "11183  1388718                                                NaN   \n",
       "11184  1388738  Intercellular is an Educational Experience Tak...   \n",
       "11185  1388746  Wonderfall Mixes VR and Actual Reality For An ...   \n",
       "11186  1388760   VR and AR Market Forecast and Future Projections   \n",
       "\n",
       "                                                    text        date  \n",
       "0      jan. according to the u. s. national institute...  2019-01-03  \n",
       "1      the npxs token developer aim to have ethereum ...  2019-01-06  \n",
       "2      pundi x (currency:npxs) traded 2. u dollar dur...  2019-01-02  \n",
       "3      pundi x (currency:npxs) traded 0. pm eastern o...  2019-01-04  \n",
       "4      pundi x (currency:npxs) traded down 0. u dolla...  2019-01-03  \n",
       "...                                                  ...         ...  \n",
       "11182  microsoft for the mixed reality partnership pr...  2019-01-02  \n",
       "11183  ar (ar) app that convert any room to a virtual...  2019-01-09  \n",
       "11184  there nothing like a good educational vr (vr) ...  2019-02-13  \n",
       "11185  it got a lot of vr and ar exhibit right now, n...  2019-01-30  \n",
       "11186  immersive technology adoption rate have steadi...  2019-01-30  \n",
       "\n",
       "[11187 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_articles_data = pd.read_csv('articles_raw.csv')\n",
    "raw_articles_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to process and tokenize raw texts\n",
    "def preprocess(text, stopwords={}, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "    # Handle URL\n",
    "    text = re.sub(r\"https?://t.co/\\w{10}\",' ', text)\n",
    "    # Deal with \"'s\" \n",
    "    text = re.sub(r\"'s\", \"\", text)\n",
    "    # Deal with \"'\" \n",
    "    translator2 = str.maketrans({key: None for key in string.punctuation[6]})\n",
    "    text = text.translate(translator2) \n",
    "    # Deal with the rest of punctuations\n",
    "    translator3 = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    text = text.translate(translator3)\n",
    "    # Handle unicode\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "    # Split the text\n",
    "    r1 = nltk.word_tokenize(text)\n",
    "    # Lemmatize the text\n",
    "    r2 = [lemmatizer.lemmatize(word) for word in r1]\n",
    "    # Remove the stopwords\n",
    "    r3 = [word for word in r2 if not word in stopwords]\n",
    "    # Remove digits\n",
    "    r4 = [word for word in r3 if word.isalpha()]\n",
    "    return r4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shamita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Shamita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shamita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "extra_stopwords = set()\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) | extra_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the preprocessed texts into a list\n",
    "articles = []\n",
    "for i in range(0,raw_articles_data.shape[0]):\n",
    "    tokenized_text = preprocess(raw_articles_data['text'][i], stopwords)\n",
    "    articles.append(' '.join(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to build a sparse TFIDF matrix\n",
    "def tfidf(docs):    \n",
    "    document_words = [doc.split() for doc in docs]\n",
    "    vocab = sorted(set(sum(document_words, [])))\n",
    "    vocab_dict = {k:i for i,k in enumerate(vocab)}\n",
    "    X_tf = np.zeros((len(docs), len(vocab)), dtype=int)\n",
    "    for i,doc in enumerate(document_words):\n",
    "        for word in doc:\n",
    "            X_tf[i, vocab_dict[word]] += 1\n",
    "            \n",
    "    idf = np.log(X_tf.shape[0]/X_tf.astype(bool).sum(axis=0))\n",
    "    X_tfidf = X_tf * idf\n",
    "    tfidf = sp.csr_matrix(X_tfidf)\n",
    "    all_words = vocab\n",
    "    return(tfidf,all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a TFIDF matrix for the texts and normalize the matrix\n",
    "X_tfidf, words = tfihdf(articles)\n",
    "tf_idf_norm = normalize(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to select the optimal k value of k-means clustering with silhouette method\n",
    "def plot_sil(df, kmax=30): \n",
    "    sil = []\n",
    "    for k in range(2, kmax+1):\n",
    "        kmeans = KMeans(n_clusters=k, max_iter=600, algorithm='auto').fit(df)\n",
    "        labels = kmeans.labels_\n",
    "        sil.append(silhouette_score(df, labels, metric='euclidean')) \n",
    "\n",
    "    plt.plot(range(2, kmax+1), sil)\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Silhouette Method')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal k value by identifying the highest score\n",
    "random_state=0xCAFE\n",
    "plot_sil(svd_matrix, kmax=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to cluster the texts and plot the position of the texts in 2D space \n",
    "def plot_and_predict(df, n_clusters=2, random_state=0xCAFE):\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, max_iter=600, algorithm='auto')\n",
    "    fitted = kmeans.fit(df)\n",
    "    prediction = kmeans.predict(df)\n",
    "    \n",
    "    plot_matrix = TruncatedSVD(n_components=2, algorithm='randomized', random_state=random_state).fit_transform(df)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    for c in range(n_clusters):\n",
    "        ax.scatter(plot_matrix[prediction==c][:,0], plot_matrix[prediction==c][:,1], label=c, alpha=0.8, edgecolors='none')\n",
    "\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the clustering result in 2D space \n",
    "prediction = plot_and_predict(svd_matrix, n_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the raw texts in these two groups\n",
    "n_clusters = 2\n",
    "for i in range(n_clusters):\n",
    "    print('%d, The raw texts in group %d:'%(i,i))\n",
    "    result = raw_articles_data[prediction==i]['text']   \n",
    "    display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with the same articles in group 1\n",
    "index, = np.where(prediction==0)\n",
    "new_raw_articles_data = raw_articles_data.iloc[index].reset_index(drop=True)\n",
    "\n",
    "tf_idf_norm_step2 = tf_idf_norm[prediction==0]\n",
    "svd_matrix_step2 = TruncatedSVD(n_components=100, algorithm='randomized', random_state=0xCAFE).fit_transform(tf_idf_norm_step2)\n",
    "\n",
    "# Determine the optimal k value by identifying the highest score\n",
    "random_state=0xCAFE\n",
    "plot_sil(svd_matrix_step2, kmax=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the clustering result in 2D space \n",
    "prediction2 = plot_and_predict(svd_matrix_step2, n_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the raw texts in these two groups\n",
    "n_clusters = 2\n",
    "for i in range(n_clusters):\n",
    "    print('%d, The raw texts in group %d:'%(i,i))\n",
    "    result = raw_articles_data[prediction==0][prediction2==i]['text']   \n",
    "    display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cluster centers\n",
    "random_state = 0xCAFE\n",
    "kmeans = KMeans(n_clusters=2, max_iter=600, algorithm='auto')\n",
    "fitted = kmeans.fit(svd_matrix_step2)\n",
    "pred = kmeans.predict(svd_matrix_step2)\n",
    "centers = fitted.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose article whose position is most close to the cluster center as representative\n",
    "closest, _ = pairwise_distances_argmin_min(centers, svd_matrix_step2)\n",
    "representative_article = new_raw_articles_data['text'][closest[0]]\n",
    "new_raw_articles_data['title'][closest[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose words with top-5 highest TFIDF scores as top-5 keywords\n",
    "tf_idf_array = tf_idf_norm_step2.toarray()\n",
    "top5_index = tf_idf_array[closest[0]].argsort()[-5:][::-1]\n",
    "top5_words = list(np.array(words)[top5_index.tolist()])\n",
    "top5_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose articles with top-10 highest sum of TFIDF scores regarding the top-5 keywords as top-10 documents\n",
    "sum_tfidf = np.sum(tf_idf_array[:,top5_index],axis=1)\n",
    "top10_index = sum_tfidf.argsort()[-10:][::-1]\n",
    "top_articles_q3 = np.array(new_raw_articles_data['text'][top10_index]).tolist()\n",
    "new_raw_articles_data['title'][top10_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose articles with top-10 highest cosine similarities regarding representative article as top-10 documents\n",
    "c_matrix = cosine_similarity(tf_idf_norm_step2)\n",
    "array1 = np.squeeze(np.asarray(c_matrix))[closest[0]]\n",
    "array2 = np.where(array1 > 0)[0]\n",
    "top10_index2 = array1[array2].argsort()[-10:][::-1]\n",
    "top_articles_q4 = np.array(new_raw_articles_data['text'][array2[top10_index2]]).tolist()\n",
    "new_raw_articles_data['title'][array2[top10_index2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose articles with top-10 highest cosine similarities only calculated by top-5 keywords as top-10 documents\n",
    "c_matrix = cosine_similarity(tf_idf_norm_step2[:,top5_index])\n",
    "array1 = np.squeeze(np.asarray(c_matrix))[closest[0]]\n",
    "array2 = np.where(array1 > 0)[0]\n",
    "top10_index3= array1[array2].argsort()[-10:][::-1]\n",
    "top_articles_q5 = np.array(new_raw_articles_data['text'][array2[top10_index3]]).tolist()\n",
    "new_raw_articles_data['title'][array2[top10_index3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the above results\n",
    "Results_Dict = {'Category 1': [representative_article, top5_words, top_articles_q3, top_articles_q4, top_articles_q5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_Dict['Category 1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_Dict['Category 1'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_Dict['Category 1'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "66. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
